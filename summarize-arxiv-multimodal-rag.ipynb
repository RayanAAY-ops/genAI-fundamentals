{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install  -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import openai, os\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "secrets= dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = secrets['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "path = \"data/\"\n",
    "\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"RAG_benchmark.pdf\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=1000, \n",
    "    new_after_n_chars=800, \n",
    "    combine_text_under_n_chars=500,\n",
    "    image_output_dir_path=path + 'images/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstract'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements[1].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.CompositeElement at 0x3783c07a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c1880>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c1790>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c0920>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c0950>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c3fe0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c09e0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c0ad0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c06b0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c0830>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x356b82720>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x356b98c20>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x356b82ab0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x356d7b620>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x356e492b0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783257f0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783256a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324da0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783241a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325700>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325550>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325850>,\n",
       " <unstructured.documents.elements.Table at 0x356d7b650>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325970>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324b00>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325af0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324800>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325a60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324830>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325d60>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325be0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324ad0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325ac0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378324650>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325cd0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325a90>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325f10>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783256d0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783250a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378326480>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378326390>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325f40>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325f70>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325fa0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783265a0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783263c0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x378325dc0>,\n",
       " <unstructured.documents.elements.CompositeElement at 0x3783c6300>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "table_elements = []\n",
    "text_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if element.category == \"Table\":\n",
    "        table_elements.append(element.text)\n",
    "    elif element.category == \"CompositeElement\":\n",
    "        text_elements.append(element.text)\n",
    "\n",
    "\n",
    "table_elements = list(filter(None, table_elements))\n",
    "text_elements = list(filter(None, text_elements))\n",
    "print(len(table_elements))\n",
    "print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import NumberedListOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = \"\"\"You are an AI PhD specialized in AI and RAG, you are tasked with summarizing tables and text. \n",
    "Give a concise summary of the table or text.\n",
    "\n",
    "Table or text chunk: {element}\n",
    "\"\"\" \n",
    "model_name='gpt-4' #\"text-embedding-3-small\" \n",
    "model = ChatOpenAI(temperature=0, model_name=model_name)\n",
    "summarize_chain = LLMChain.from_string(\n",
    "    llm=model,\n",
    "    template=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'element': 'Benchmark Web KG Mock Dynamic Torso and Beyond retrieval search API question tail facts Wikipedia QALD-10 [24] x v x x x x MS MARCO [3] v x x not explicitly not explicitly v Natural Questions [12] v x x not explicitly not explicitly x RGB [5] v x x x x v FreshLLM [25] x x x v x v CRAG v v v v v v',\n",
       "  'text': \"The table provides a comparison of different AI systems in terms of their capabilities. The systems compared are QALD-10 [24], MS MARCO [3], Natural Questions [12], RGB [5], FreshLLM [25], and CRAG. The features compared include Benchmark Web, KG Mock, Dynamic Torso, Beyond retrieval, search API, question tail, facts, and Wikipedia. CRAG is the only system that supports all the features. MS MARCO [3] and Natural Questions [12] do not explicitly support 'Beyond retrieval' and 'search API'. FreshLLM [25] supports 'Dynamic Torso' and 'facts' but not the other features. RGB [5] does not support 'KG Mock' and 'Dynamic Torso'. QALD-10 [24] only supports 'Benchmark Web'.\"},\n",
       " {'element': 'Question type Definition Simple Questions asking for simple facts that are unlikely to change overtime, such as the birth date of a person and the authors of a book. Simple w. Condition Questions asking for simple facts with some given conditions, such as stock prices on a certain date and a director’s recent movies in a certain genre. Set Questions that expect a set of entities or objects as the answer (e.g., “what are the continents in the southern hemisphere?”). Comparison Questions that compare two entities (e.g., “who started performing earlier, Adele or Ed Sheeran?”). Aggregation Questions that require aggregation of retrieval results to answer (e.g., “how many Oscar awards did Meryl Streep win?’). Multi-hop Questions that require chaining multiple pieces of information to compose the answer (e.g., “who acted in Ang Lee’s latest movie?’”). Post-processing Questions that need reasoning or processing of the retrieved information to obtain heavy the answer (e.g., “how many days did',\n",
       "  'text': 'The text chunk categorizes different types of questions. Simple questions ask for unchanging facts like birth dates or authors of a book. Simple with condition questions ask for facts under specific conditions, like stock prices on a certain date. Set questions expect a set of entities or objects as the answer. Comparison questions compare two entities. Aggregation questions require aggregation of retrieval results to answer. Multi-hop questions require chaining multiple pieces of information to compose the answer. Post-processing questions need reasoning or processing of the retrieved information to obtain the answer.'},\n",
       " {'element': \"Thurgood Marshall serve as a Supreme Court justice ?”). False Premise Questions that have a false preposition or assumption (e.g., “What’s the name of Taylor Swift's rap album before she transitioned to pop?” (Taylor Swift has not yet released any rap album)).\",\n",
       "  'text': \"The text discusses two types of questions: one that asks about a true fact, using Thurgood Marshall's service as a Supreme Court justice as an example, and another that is based on a false premise, exemplified by a hypothetical question about a non-existent rap album by Taylor Swift before her pop career.\"},\n",
       " {'element': 'Dynamism Finance Sports Music Movie Open Total Real-time 434 (42) 0( 0) 2( 0) 0( 0) 1( 0) 437 (10) Fast-changing 204 (20) 275(33) 40( 6) 17( 2) 28 ( 4) 564 (13) Slow-changing 183 (18) 215 (26) 152 (24) 253 (22) 204(26) 1,007 (23) Static 218 (21) 343(41) 430 (69) 855 (76) 555 (70) 2,401 (54) All 1,039 833 624 1,125 788 4,409',\n",
       "  'text': 'The table presents data on the dynamism of different sectors: Finance, Sports, Music, and Movies. The categories of dynamism are Real-time, Fast-changing, Slow-changing, and Static. The highest number of real-time changes occur in Finance (434), while the least is in Sports, Music, and Movies (0 each). Fast-changing dynamics are most prevalent in Finance (204) and least in Music (17). Slow-changing dynamics are most common in Music (253) and least in Sports (152). Static changes are most common in Movies (855) and least in Sports (430). The total number of changes across all sectors is highest for Static changes (2,401) and lowest for Real-time changes (437).'},\n",
       " {'element': 'Question type Finance Sports Music Movie Open Total Simple 466 (45) 23( 3) 112 (18) 519 (46) 85 (11) 1,205 (27) Simple w. condition 113(11) 250(30) =92 (15) 112(10) 122 (15) 689 (16) Set 48 ( 5) 93(11) = 72 (12) 104 ( 9) 86 (11) 403 ( 9) Comparison 146 (14) 85 (10) 102 (16) 105(.9) 98 (12) 536 (12) Aggregation 69( 7) 137(16) 96 (15) 71( 6) 116(15) 489 (11) Multi-hop 86 ( 8) 64( 8) 55( 9) 90 ( 8) 87 (11) 382 ( 9) Post-processing heavy 26 ( 3) 24( 3) 26( 4) 28 ( 2) 76 (10) 180( 4) False Premise 85( 8) 157(19) = 69 (11) 96( 9) 118(15) 525 (12) All 1,039 833 624 1,125 788 4,409',\n",
       "  'text': \"The table presents the distribution of different types of questions across five categories: Finance, Sports, Music, Movie, and Open. The categories with the highest number of questions are Finance (1,039 questions) and Movie (1,125 questions). The most common type of question across all categories is 'Simple', accounting for 27% of all questions. The least common type is 'Post-processing heavy', making up only 4% of all questions. Other types of questions include 'Simple with condition', 'Set', 'Comparison', 'Aggregation', 'Multi-hop', and 'False Premise'.\"},\n",
       " {'element': 'Model Accuracy Hallucination Missing Score, LLM only — Llama 3 70B Instruct 32.3 28.9 38.8 3.4 GPT-4 Turbo 33.5 13.5 53.0 20.0 Task 1 Llama 3 70B Instruct 35.6 31.1 33.3 4.5 GPT-4 Turbo 35.9 28.2 35.9 77 Task 2 Llama 3 70B Instruct 37.5 29.2 33.3 8.3 GPT-4 Turbo 413 25.1 33.6 16.2 Task 3 Llama 3 70B Instruct 40.6 31.6 27.8 9.1 GPT-4 Turbo 43.6 30.1 26.3 13.4',\n",
       "  'text': 'The table compares the performance of two AI models, LLM only — Llama 3 70B Instruct and GPT-4 Turbo, across three tasks. The metrics used are Accuracy, Hallucination, Missing Score. In all tasks, GPT-4 Turbo consistently outperforms Llama 3 70B Instruct in terms of accuracy. However, Llama 3 70B Instruct has a lower hallucination rate in all tasks. The missing score is relatively similar for both models across all tasks.'},\n",
       " {'element': 'System Perfect Acc. Hall. Miss. Score; Latency (ms) Equal Copilot Pro 626 11.7 17.9 7.8 50.6 11,596 weighted Gemini Advanced 60.8 10.1 16.6 12.5 49.3 5,246 ChatGPT Plus 59.8 13.1 25.2 1.9 41.2 6,195 Perplexity.ai 55.8 8.8 25.3 10.1 34.9 2,455 Traffic Copilot Pro 60.7. 14.2 18.5 6.7 49.3 - weighted Gemini Advanced 59.1 10.9 16.6 13.4 48.0 - ChatGPT Plus 59.7 13.3. 25.1 1.9 41.3 - Perplexity.ai 55.8 9.7 25.2 9.3 35.5 -',\n",
       "  'text': 'The table presents performance metrics for four AI systems: Equal Copilot Pro, Gemini Advanced, ChatGPT Plus, and Perplexity.ai. The metrics include Perfect Accuracy (Acc.), Hallucinations (Hall.), Misses (Miss.), Score, and Latency. \\n\\nEqual Copilot Pro has the highest score (50.6) and perfect accuracy (626), but also the highest latency (11,596 ms). Gemini Advanced and Traffic Copilot Pro have similar scores (49.3), with Gemini having lower latency (5,246 ms). ChatGPT Plus has a lower score (41.2) and moderate latency (6,195 ms), while Perplexity.ai has the lowest score (34.9) and the lowest latency (2,455 ms). \\n\\nIn the Traffic Copilot Pro category, the scores are similar to the previous category, but latency is not provided. The scores range from 35.5 (Perplexity.ai) to 49.3 (Traffic Copilot Pro).'},\n",
       " {'element': \"Dynamism Definition The answer to the question changes over seconds Real-time (e.g., “What's Costco’s stock price today?”). The answer to the question changes no more than daily Fast-changing (e.g., “When is Laker’s game tonight?”). The answer to the question changes no more than yearly Slow-changing (e.g., “Who won the Grammy award last year?”). The answer to the question does not change over time, Static such as the birth date of a person.\",\n",
       "  'text': \"The text describes different levels of dynamism in information. Real-time dynamism involves changes every few seconds, such as Costco's stock price. Fast-changing dynamism involves daily changes, like the schedule of a Laker's game. Slow-changing dynamism involves yearly changes, such as the winner of the Grammy award last year. Static information does not change over time, like a person's birth date.\"},\n",
       " {'element': 'Key Value \"page name\" \"A Short History Of ChatGPT: How We Got To Where We Are Today\" \"page url\" \"https://www.forbes.com/sites/bernardmart/2023/05/19/a-short-history-of-chatgpt- how...\" \"page snippet\" \"OpenAI released an early demo of ChatGPT on <strong>November 30, 2022</strong>...\" \"page last modified\" \"2024-1-18 15:32:24\" \"html page\" \"<!IDOCTYPE html><html lang=\"en\"><head><link rel=\"preload\" as=\"font\" href=\"https...\"',\n",
       "  'text': 'The webpage titled \"A Short History Of ChatGPT: How We Got To Where We Are Today\" is available at the URL \"https://www.forbes.com/sites/bernardmart/2023/05/19/a-short-history-of-chatgpt- how...\". The page, last modified on January 18, 2024, contains information about the early demo of ChatGPT released by OpenAI on November 30, 2022.'},\n",
       " {'element': 'Web Questions KG Questions Question Type Est. Recall Count Est. Recall Count Simple 94 160 60 1,045 Simple w. Condition 89 420 81 269 Set 87 326 23 186 Comparison 97 350 84 158 Aggregation 95 331 91 149 Multi-hop 91 312 83 71 Post-processing heavy 75 150 90 30 False Premise 49 376 0 70 All 84 = 2,425 63 1,984',\n",
       "  'text': \"The table presents the estimated recall count for different types of questions in Web Questions and KG Questions. For Web Questions, the highest recall count is for 'Simple' questions at 160, followed by 'Comparison' questions at 350. For KG Questions, the highest recall count is for 'Simple' questions at 1,045, followed by 'Simple w. Condition' questions at 269. The lowest recall count for both types of questions is for 'Post-processing heavy' and 'False Premise' respectively. The total estimated recall count is 2,425 for Web Questions and 1,984 for KG Questions.\"},\n",
       " {'element': 'Accuracy Precision Recall F1 score ChatGPT = Llama 3 ChatGPT ~—_ Llama3 ChatGPT — Llama 3 ChatGPT = Llama 3 Accurate 94.1 98.6 98.8 98.5 92.2 99.3 92.0 98.9 Incorrect 94.1 98.6 86.8 98.7 97.8 97.2 92.0 97.9 Missing 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 Average 96.1 99.1 95.2 99.1 96.7 98.8 94.7 98.9',\n",
       "  'text': \"The table presents the performance metrics of two AI models, ChatGPT and Llama 3, in terms of accuracy, precision, recall, and F1 score. The metrics are evaluated under three conditions: accurate, incorrect, and missing. Both models perform exceptionally well with 100% scores in all metrics under the 'missing' condition. On average, both models have high performance with ChatGPT averaging 96.1% accuracy, 99.1% precision, 95.2% recall, and 99.1% F1 score. Llama 3 averages slightly higher with 96.7% accuracy, 98.8% precision, 94.7% recall, and 98.9% F1 score.\"},\n",
       " {'element': 'Model Accuracy (%) Hallucination (%) — Missing (%) Score (%) — Latency (ms) LLM only — Llama 2 7B Chat 14.8 78.4 6.7 -63.6 289 Llama 2 70B Chat 22.3 28.7 49.0 6.4 1,392 Llama 3 8B Instruct 23.7 33.8 42.6 -10.1 823 Llama 3 70B Instruct 32.3 28.9 38.8 34 1,252 GPT-4 Turbo 33.5 13.5 53.0 20.0 619 Task 1 Llama 2 7B Chat 16.4 83.1 0.5 -66.7 1,545 Llama 2 70B Chat 29.3 61.0 9.7 “31.7 5,229 Llama 3 8B Instruct 28.5 45.6 25.9 “17.1 2,152 Llama 3 70B Instruct 35.6 311 33.3 45 2,481 GPT-4 Turbo 35.9 28.2 35.9 17 1,270 Task 2 Llama 2 7B Chat 16.4 83.1 0.5 -66.7 1,569 Llama 2 70B Chat 29.1 61.1 9.7 -32.0 5,251 Llama 3 8B Instruct 28.6 45.5 25.9 -16.9 1,701 Llama 3 70B Instruct 375 29.2 33.3 8.3 2,175 GPT-4 Turbo 413 25.1 33.6 16.2 1,074 Task 3 Llama 2 7B Chat 16.0 83.6 04 -67.6 2,142 Llama 2 70B Chat 319 65.7 24 -33.7 6,149 Llama 3 8B Instruct 32.1 56.3 11.6 -24.1 4,536 Llama 3 70B Instruct 40.6 31.6 278 91 5,028 GPT-4 Turbo 43.6 30.1 26.3 14 1,688',\n",
       "  'text': 'The table presents the performance of different AI models, including Llama 2 7B Chat, Llama 2 70B Chat, Llama 3 8B Instruct, Llama 3 70B Instruct, and GPT-4 Turbo, across three tasks. The performance metrics include model accuracy, hallucination, missing, score, and latency. \\n\\nIn general, GPT-4 Turbo consistently outperforms the other models in terms of accuracy and latency across all tasks. Llama 2 7B Chat tends to have the highest hallucination rates and negative scores, while also having the lowest accuracy. The 70B versions of the Llama models generally perform better than their 7B and 8B counterparts. The latency is highest for the Llama 2 70B Chat model across all tasks.'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_chain.batch(table_elements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
